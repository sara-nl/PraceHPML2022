{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all packages we need and set the correct device. If a CUDA compatible GPU is found, it will be used. If not, everything will be done on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the data\n",
    "\n",
    "We make two classes for processing our data. The first class, TextEncoder, will be used to encode and decode text data, since we cannot use strings as input directly. All unique words will be found and mapped to an integer. Apart from the unique words we extract from the data, TextEncoder will also have an unknown token: [UNK]. This token will be used if, during inference, we encounter a word that is not part of our vocabulary.\n",
    "\n",
    "Our second class, TextData, will handle the reading and sampling of our input data. It will use and instance of the TextEncoder class to encode our data. TextData also lets us sample sequences of text. In order to do this, TextData needs an implementation of the \\_\\_getitem\\_\\_ method, which will tell it how to handle indices. We also need a \\_\\_len\\_\\_ method so that our TextData class can work with pytorch's DataLoader, but more on that later.\n",
    "\n",
    "All you really need to understand for now though, is that these two classes read data from a txt file and encode it so we can use it for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder():\n",
    "    def __init__(self, file_path):\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.encoder = dict()\n",
    "        self.decoder = dict()\n",
    "\n",
    "        self._extract_vocab(file_path)\n",
    "        self._make_encoder_decoder()\n",
    "    \n",
    "    def _extract_vocab(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            vocab = text.split()\n",
    "            vocab = [re.sub('[^A-Za-z0-9]+', '', word.lower()) for word in vocab if word != \"\"]\n",
    "            vocab = set(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) + 1 # add one for unknown word token\n",
    "\n",
    "    def _make_encoder_decoder(self):\n",
    "        word_ids = range(1, self.vocab_size) # reserve 0 for unknown words\n",
    "        self.encoder = dict(zip(self.vocab, word_ids))\n",
    "        self.decoder = dict(zip(word_ids, self.vocab))\n",
    "\n",
    "        # add unknown token and id\n",
    "        self.encoder[\"[UNK]\"] = 0\n",
    "        self.decoder[0] = \"[UNK]\"\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, file_path, text_encoder, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text = self._read_text(file_path)\n",
    "        self.encoded_text = self.encode_text(self.text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.seq_len - 2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"sequence\": self.encoded_text[index : index + self.seq_len],\n",
    "            \"next_tokens\": self.encoded_text[index + 1 : index + self.seq_len + 1],\n",
    "        }\n",
    "\n",
    "    def _read_text(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        all_words = text.split()\n",
    "        all_words = [word.lower() for word in all_words if word != \"\"]\n",
    "        encoded_words = [self.text_encoder.encoder[re.sub('[^A-Za-z0-9]+', '', word.lower())] if word in self.text_encoder.vocab else self.text_encoder.encoder[\"[UNK]\"] for word in all_words]\n",
    "\n",
    "        return np.asarray(encoded_words)\n",
    "\n",
    "    def decode_text(self, tokens):\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            sentence.append(self.text_encoder.decoder[token])\n",
    "\n",
    "        return \" \".join(sentence)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Once we can process our data, we need a model. In this notebook we will train an LSTM with pytorch, which we define as a class here. We choose which layers we want our model to have and define the forward pass, as well as a function that generates text.\n",
    "\n",
    "The model consists of three parts: \n",
    "\n",
    "<ol>\n",
    "    <li> Embedding layer. This part learns vector representations for each word in the vocabulary.\n",
    "    <li> LSTM. This is the recurrent part that models text.\n",
    "    <li> Linear classifier. This is an extra layer after the LSTM that makes the prediction.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        # embedding\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        # recurrent network\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        # classification head\n",
    "        self.linear_classifier = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, previous_state=None):\n",
    "        embeddings = self.embedding_layer(x)\n",
    "        h_all, (h_n, c_n) = self.lstm(embeddings, previous_state)\n",
    "        logits_all = self.linear_classifier(h_all)\n",
    "\n",
    "        # make sure to take the last layer output for classification\n",
    "        logits_n = logits_all[:,-1,:].squeeze(1)\n",
    "        logits_all = logits_all.permute(0,2,1)\n",
    "        \n",
    "        # in the case we are generating we have to keep track of the hidden (cell) state\n",
    "        if previous_state is not None:\n",
    "            return logits_n, (h_n, c_n)\n",
    "\n",
    "        return logits_all, logits_n\n",
    "    \n",
    "    def generate(self, x, gen_len=32, sample=False, temperature=1.0):\n",
    "\n",
    "        for i in range(0, gen_len):\n",
    "\n",
    "            if i == 0:\n",
    "                logits_n, (h_n, c_n) = self.forward(x, previous_state=self.init_state())\n",
    "            else:\n",
    "                logits_n, (h_n, c_n) = self.forward(x, previous_state=(h_n, c_n))\n",
    "\n",
    "            # control how random we want to be in our sampling\n",
    "            logits_n = logits_n / temperature\n",
    "\n",
    "            # get a probability distribution over your outputs (vocabulary)\n",
    "            probs = torch.softmax(logits_n, dim=1)\n",
    "\n",
    "            if sample:\n",
    "                next_word = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_word = torch.argmax(probs, dim=1)    \n",
    "            \n",
    "            # add the next word to your train of token inputs and repeat the process\n",
    "            next_word = next_word.unsqueeze(1)\n",
    "            x = torch.cat((x, next_word), dim=1)\n",
    "\n",
    "        return x.squeeze(0).cpu().tolist()\n",
    "\n",
    "\n",
    "    def init_state(self):\n",
    "        \"\"\"\n",
    "        When generating we want to initialize the hidden (cell) states to 0.\n",
    "        See it as generating from a blank slate.\n",
    "        \"\"\"\n",
    "        return (torch.zeros(self.lstm.num_layers, 1, self.lstm.hidden_size).to(device),\n",
    "                torch.zeros(self.lstm.num_layers, 1, self.lstm.hidden_size).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here we set some hyperparameters that will define how the model is trained. You can leave them as is or play with them and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "SEQUENCE_LEN = 32\n",
    "HIDDEN_DIM = 512\n",
    "EMBEDDING_DIM = 32\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some functions\n",
    "\n",
    "The most important function we're creating here is train\\_loop, as this is how we train our model. The generate function is used to generate text. It is called at the end of every epoch, so we can see the model improve. The last function here is print\\_model\\_size, which we simply add to give you a sense of how big such a model is. If you do play around with the hyperparameters above, you can see how this influences not only the performance, but also the size. \n",
    "\n",
    "We add a separate cell where you can change the DATA\\_FILE\\_PATH to any of the txt files in the Data folder, that way you can choose which data you want to train on. You can also type the prompt that is used to generate a sentence each epoch. Keep in mind the model will only recognize words it has seen in the txt file you train on!\n",
    "\n",
    "We have the following txt files available:\n",
    "\n",
    "<ul>\n",
    "    <li> alice_in_wonderland.txt\n",
    "    <li> dummy_text.txt\n",
    "    <li> frankenstein.txt\n",
    "    <li> romeo_and_juliet.txt\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"./Data/dummy_text.txt\"\n",
    "PROMPT = \"It is a long established fact that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            model.train()\n",
    "\n",
    "            input_sequence = batch[\"sequence\"].to(device)\n",
    "            next_tokens = batch[\"next_tokens\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(input_sequence)\n",
    "            loss = loss_function(outputs, next_tokens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if i % 9 == 0:\n",
    "                print(f\"[{epoch + 1}, {i + 1:5d}] train loss: {np.mean(losses):.3f}\")\n",
    "        \n",
    "        print(generate_sentence(model, dataloader, PROMPT, 32))\n",
    "    print(\"Finished training.\")\n",
    "\n",
    "def generate_sentence(model, dataloader, sentence, gen_len):\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize your prompt text\n",
    "    x = torch.tensor([dataloader.dataset.encode_text(sentence)]).to(device)\n",
    "\n",
    "    # generate tokens\n",
    "    generated_tokens = model.generate(x, gen_len=gen_len, sample=False, temperature=1.0)\n",
    "\n",
    "    # decode the tokens back to normal text\n",
    "    sentence = dataloader.dataset.decode_text(generated_tokens)\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement()*param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement()*buffer.element_size()\n",
    "\n",
    "    size = (param_size + buffer_size) / 1024**2\n",
    "    print(\"model number of params: \", sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "    print(\"model size: {:.3f}MB\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your LSTM!\n",
    "\n",
    "Here we call the individual parts we defined above and actually do the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(DATA_FILE_PATH)\n",
    "dataset = TextData(DATA_FILE_PATH, text_encoder, SEQUENCE_LEN)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE)\n",
    "\n",
    "model = LSTM(vocab_size=text_encoder.vocab_size, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS).to(device)\n",
    "print(model)\n",
    "print_model_size(model)\n",
    "\n",
    "train_loop(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a836eeef6cdcf90e792b78663a480313626ae51413242fef7331128020a76261"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

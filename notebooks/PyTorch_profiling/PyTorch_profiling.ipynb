{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63eac11",
   "metadata": {},
   "source": [
    "# Profiling with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ebf3793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Sequence, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchmetrics.functional as metrics\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_PATH = os.getenv('TEACHER_DIR', os.getcwd()) + '/JHL_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97482e",
   "metadata": {},
   "source": [
    "## What is profiling?\n",
    "\n",
    "According to [wikipedia](https://en.wikipedia.org/wiki/Profiling_(computer_programming)): \n",
    "\n",
    "\"Profiling is a form of dynamic program analysis that measures, for example, the space (memory) or time complexity of a program, the usage of particular instructions, or the frequency and duration of function calls. Most commonly, profiling information serves to aid program optimization, and more specifically, performance engineering.\"\n",
    "\n",
    "What this means is that you analyse your program, trying to identify bottlenecks, and thereby optimizing it's execution. As an example, you might have an application needs to read a lot of input data (quite typical in machine learning!) during its run. A profile might show you that while your code runs, your processor is mostly idling since it is waiting for input data. This might give you a hint on how to optimize your program: maybe you can read in _part_ of the input, and already start computing on that _while_ you're loading in your next samples. Or: maybe you can copy your data to a faster disk, before you start running.\n",
    "\n",
    "## Why should I care about profiling?\n",
    "\n",
    "You may know that training large models like GPT-3 takes several _million_ dollars [source](https://lambdalabs.com/blog/demystifying-gpt-3/) and a few hundred MWh [source](https://www.theregister.com/2020/11/04/gpt3_carbon_footprint_estimate/). If the engineers that trained these models did _not_ spend time on optimization, it might have been several million dollars and hunderds of MWh more.\n",
    "\n",
    "Sure, the model you'd like to train is probably not quite as big. But maybe you want to train it 10000 times, because you want to do hyperparameter optimization. And even if you only train it once, it may take quite a bit of compute resources, i.e. money and energy.\n",
    "\n",
    "## When should I care about profiling?\n",
    "\n",
    "Well, you should _always_ care if your code runs efficiently, but there's different levels of caring.\n",
    "\n",
    "From personal experience: if I know I'm going to run a code only once, for a few days, on a single GPU, I'll probably not create a full profile. What I _would_ do is inspect my GPU and CPU utilization during my runs, just to see if it is _somewhat_ efficient, and if I didn't make any obvious mistakes (e.g. accidentally _not_ using the GPU, even if I have one available).\n",
    "\n",
    "If I know that I'll run my code on multiple GPUs, for multiple days, (potentially) on multiple nodes, and/or I need to run it multiple times, I know that my resource footprint is going to be large, and it's worth spending some time and effort to optimize the code. That's when I'll create a profile. The good part is: the more often you do it, the quicker and more adapt you become at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ead022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, log_interval=10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move data and target to the gpu, if available and used\n",
    "        data, target = map(lambda tensor: tensor.to(device, non_blocking=True), (data, target))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        accuracy = metrics.accuracy(output, target)\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100 * batch_idx / len(train_loader):.0f}%)]'\n",
    "                f'\\tLoss: {loss.detach().item():.6f}'\n",
    "                f'\\tAccuracy: {accuracy.detach().item():.2f}'\n",
    "            )\n",
    "\n",
    "        yield loss.detach().item(), accuracy.detach().item() \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        # move data and target to the gpu, if available and used\n",
    "        data, target = map(lambda tensor: tensor.to(device, non_blocking=True), (data, target))\n",
    "\n",
    "        # get model output\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "        # get most likely class label\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "\n",
    "        # count the number of correct predictions\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    yield test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49953023",
   "metadata": {},
   "source": [
    "### Function to run training and testing loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09176b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, optimizer, n_epochs, device, train_loader, test_loader, log_interval):\n",
    "\n",
    "    # get the validation loss and accuracy of the untrained model\n",
    "    start_val_loss, start_val_acc = tuple(test(model, device, test_loader))[0]\n",
    "\n",
    "    # don't mind the following train/test loop logic too much, if you want to know what's happening, let us know :)\n",
    "    # normally you would pass a logger to your train/test loops and log the respective metrics there\n",
    "    (train_loss, train_acc), (val_loss, val_acc) = map(lambda arr: np.asarray(arr).transpose(2,0,1), zip(*[\n",
    "        (\n",
    "            [*train(model, device, train_loader, optimizer, epoch, log_interval)],\n",
    "            [*test(model, device, test_loader)]\n",
    "        )\n",
    "        for epoch in range(n_epochs)\n",
    "    ]))\n",
    "\n",
    "    # flatten the arrays\n",
    "    train_loss, train_acc, val_loss, val_acc = map(np.ravel, (train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    # prepend the validation loss and accuracy of the untrained model\n",
    "    val_loss, val_acc = (start_val_loss, *val_loss), (start_val_acc, *val_acc)\n",
    "\n",
    "    plot_metric_curve(train_loss, val_loss, n_epochs, 'Loss')\n",
    "    plot_metric_curve(train_acc, val_acc, n_epochs, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186959d",
   "metadata": {},
   "source": [
    "# Defining the model\n",
    "Moving from a FCNN to a CNN, we split our model in two: a feature extractor and a classifier.  \n",
    "The classifier will be no different to the FCNN: Some linear layers with a non-linearity in-between.  \n",
    "The feature extractor will be our convolutional layers, with a downsampling operation after each layer; in our case max-pooling.  \n",
    "(Which you shouldn't normally use unless you have a very good reason to! Take it from Saint Geoff:\n",
    "https://mirror2image.wordpress.com/2014/11/11/geoffrey-hinton-on-max-pooling-reddit-ama/) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f7a452",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR10CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 4 convolution layers, with a non-linear activation after each.\n",
    "        # maxpooling after the activations of the 2nd, 3rd, and 4th conv layers\n",
    "        # 2 dense layers for classification\n",
    "        # log_softmax\n",
    "        #\n",
    "        # As for the number of channels of each layers, try to experiment!\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        # in_features of the first layer should be the product of the output shape of your feature extractor!\n",
    "        # E.g. if the output of your feature extractor has size (batch x 128 x 4 x 4), in_features = 128*4*4=2048\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=2048, out_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=2048, out_features=10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        return self.classifier(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4187106",
   "metadata": {},
   "source": [
    "### Function to plot train/validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe73d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_curve(\n",
    "    train_metric: Sequence[float], \n",
    "    val_metric: Sequence[float],\n",
    "    n_epochs: int,\n",
    "    metric_name: str, # Label of the y-axis, e.g. 'Accuracy'\n",
    "    x_axis_name: str = 'Epoch'\n",
    "):\n",
    "    # create values for the x-axis\n",
    "    train_steps, val_steps = map(\n",
    "        lambda metric_values: np.linspace(start=0, stop=n_epochs, num=len(metric_values)),\n",
    "        (train_metric, val_metric)\n",
    "    )\n",
    "    \n",
    "    plt.plot(train_steps, train_metric, label='train')\n",
    "    plt.plot(val_steps, val_metric, label='validation')\n",
    "    plt.title(f\"{metric_name} vs. {x_axis_name}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadac010",
   "metadata": {},
   "source": [
    "### PyTorch boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69ffa28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA is {'' if use_cuda else 'not '}available\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.set_per_process_memory_fraction(0.22)\n",
    "\n",
    "#cpu_count = len(os.sched_getaffinity(0))\n",
    "cpu_count=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a677fc",
   "metadata": {},
   "source": [
    "### Results\n",
    "Achieving ~75% validation accuracy with our simple CNN Classifier should be possible, how did you do? Let us know!\n",
    "\n",
    "- Do you see a difference in train accuracy and validation accuracy?\n",
    "- How does the difference between training and validation accuracy change over time?\n",
    "\n",
    "The State-of-the-art is currently 99.5% accuracy! (See https://paperswithcode.com/sota/image-classification-on-cifar-10)\n",
    "\n",
    "One of the areas of improvement is that our model is still not very deep, modern models usually range from 18-50 layers.\n",
    "However, after around ~15 layers, you start to run into some issues with information propagation through your model: the gradient of the loss is not able to reach the first few layers of the model.\n",
    "\n",
    "Checkout https://arxiv.org/pdf/1512.03385.pdf for the seminal paper fixing this issue.\n",
    "\n",
    "### Next chapter: So we can classify images, but can we also generate new ones? Introducing the VAE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39cdc0",
   "metadata": {},
   "source": [
    "## Train with profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33e6f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_profiling(model, device, train_loader, optimizer, epoch, log_interval, logdir):\n",
    "    model.train()\n",
    "    \n",
    "    # Create a torch.profiler.profile object, and call it as the last part of the training loop\n",
    "    with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA],\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=1,\n",
    "        active=10),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(logdir, worker_name='worker0'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,  # This will take 1 to 2 minutes. Setting it to False could greatly speedup.\n",
    "    with_stack=True\n",
    ") as p:\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # move data and target to the gpu, if available and used\n",
    "            data, target = map(lambda tensor: tensor.to(device, non_blocking=True), (data, target))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            accuracy = metrics.accuracy(output, target)\n",
    "        \n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100 * batch_idx / len(train_loader):.0f}%)]'\n",
    "                    f'\\tLoss: {loss.detach().item():.6f}'\n",
    "                    f'\\tAccuracy: {accuracy.detach().item():.2f}'\n",
    "                )\n",
    "\n",
    "            yield loss.detach().item(), accuracy.detach().item() \n",
    "            \n",
    "            p.step()\n",
    "            \n",
    "@torch.no_grad()\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        # move data and target to the gpu, if available and used\n",
    "        data, target = map(lambda tensor: tensor.to(device, non_blocking=True), (data, target))\n",
    "\n",
    "        # get model output\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate loss\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "\n",
    "        # get most likely class label\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "\n",
    "        # count the number of correct predictions\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    yield test_loss, correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47044453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_profiling(model, optimizer, n_epochs, device, train_loader, test_loader, log_interval, logdir):\n",
    "\n",
    "    # get the validation loss and accuracy of the untrained model\n",
    "    start_val_loss, start_val_acc = tuple(test(model, device, test_loader))[0]\n",
    "\n",
    "    # don't mind the following train/test loop logic too much, if you want to know what's happening, let us know :)\n",
    "    # normally you would pass a logger to your train/test loops and log the respective metrics there\n",
    "    (train_loss, train_acc), (val_loss, val_acc) = map(lambda arr: np.asarray(arr).transpose(2,0,1), zip(*[\n",
    "        (\n",
    "            [*train_profiling(model, device, train_loader, optimizer, epoch, log_interval, logdir)],\n",
    "            [*test(model, device, test_loader)]\n",
    "        )\n",
    "        for epoch in range(n_epochs)\n",
    "    ]))\n",
    "\n",
    "    # flatten the arrays\n",
    "    train_loss, train_acc, val_loss, val_acc = map(np.ravel, (train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "    # prepend the validation loss and accuracy of the untrained model\n",
    "    val_loss, val_acc = (start_val_loss, *val_loss), (start_val_acc, *val_acc)\n",
    "\n",
    "    plot_metric_curve(train_loss, val_loss, n_epochs, 'Loss')\n",
    "    plot_metric_curve(train_acc, val_acc, n_epochs, 'Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557eeaf",
   "metadata": {},
   "source": [
    "Here, we create our own dataset object, that reads png files from a directory, and the labels from a single pkl. This pkl contains a dictionary where the filenames are keys, and the labels ar the values. Thus, to get the label from the dictionary, we can simply index it with the base filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d83e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "        \n",
    "class Cifar10PNGDataset(Dataset):\n",
    "    def __init__(self, label_file, img_dir, transform=None, target_transform=None):\n",
    "        # Load labels\n",
    "        with open(label_filename, 'rb') as fo:\n",
    "            self.label_dict = pickle.load(fo, encoding='bytes')\n",
    "        # List filenames with png extension\n",
    "        self.img_filenames = glob.glob(os.path.join(img_dir, '*.png'))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get filename with index idx:\n",
    "        img_filename = self.img_filenames[idx]\n",
    "        # Read file from disk\n",
    "        # image = read_image(img_filename)\n",
    "        image = Image.open(img_filename)\n",
    "        # Read label from label dictionary\n",
    "        label = self.label_dict[os.path.basename(img_filename)]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b68b1",
   "metadata": {},
   "source": [
    "Let's create the training dataset, and display one sample with its label to verify we did everything correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eebabd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n",
      "(0 = airplane, 1 = automobile, 2 = bird, 3 = cat, 4 = deer, 5 = dog, 6 = frog, 7 = horse, 8 = ship, 9 = truck)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "PNG_DATA = os.path.join(DATA_PATH, 'cifar10_png')\n",
    "label_filename = os.path.join(DATA_PATH, \"labels.pkl\")\n",
    "train_dataset = Cifar10PNGDataset(label_file = label_filename, img_dir = PNG_DATA)\n",
    "\n",
    "# Inspect one image and one label as example:\n",
    "img_sample = train_dataset[0][0]\n",
    "label_sample = train_dataset[0][1]\n",
    "\n",
    "#plt.imshow(img_sample.permute(1,2,0))\n",
    "plt.imshow(img_sample)\n",
    "print(f\"Label: {label_sample}\")\n",
    "print(f\"(0 = airplane, 1 = automobile, 2 = bird, 3 = cat, 4 = deer, 5 = dog, 6 = frog, 7 = horse, 8 = ship, 9 = truck)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79257a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "\n",
      "Test set: Average loss: 2.3028, Accuracy: 967/10000 (10%)\n",
      "\n",
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.302634\tAccuracy: 0.10\n",
      "Train Epoch: 0 [6000/50000 (12%)]\tLoss: 2.265243\tAccuracy: 0.15\n",
      "Train Epoch: 0 [12000/50000 (24%)]\tLoss: 2.180698\tAccuracy: 0.28\n",
      "Train Epoch: 0 [18000/50000 (36%)]\tLoss: 2.067499\tAccuracy: 0.30\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 600\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "LOGGING_INTERVAL = 10  # Controls how often we print the progress bar\n",
    "\n",
    "model = CIFAR10CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) # optim.<OPTIMIZER_FLAVOUR>(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the data to 0 mean and 1 standard deviation, now for all channels of RGB\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "PNG_DATA = os.path.join(DATA_PATH, 'cifar10_png')\n",
    "label_filename = os.path.join(DATA_PATH, \"labels.pkl\")\n",
    "train_dataset = Cifar10PNGDataset(label_file = label_filename, img_dir = PNG_DATA, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda,\n",
    "        shuffle=True,\n",
    "        num_workers=cpu_count\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(DATA_PATH, train=False, transform=transform, download=True),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=use_cuda,\n",
    "        shuffle=False,\n",
    "        num_workers=cpu_count\n",
    ")\n",
    "\n",
    "#train_profiling(model, device, train_loader, optimizer, 0, log_interval=10)\n",
    "\n",
    "logdir = \"logs/baseline_1_40/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "fit_profiling(model, optimizer, EPOCHS, device, train_loader, test_loader, LOGGING_INTERVAL, logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522be545",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "import random, os\n",
    "\n",
    "rand_port = random.randint(6000, 8000)\n",
    "username = os.getenv('USER')\n",
    "#full_logdir=os.path.join(os.getcwd(), logdir)\n",
    "\n",
    "# Kill old TensorBoard sessions\n",
    "!kill -9 $(pgrep -u $username tensorboard)\n",
    "\n",
    "%reload_ext tensorboard\n",
    "# Run with --load_fast=false, since current TensorBoard version has an issue with the profiler plugin\n",
    "# (more info https://github.com/tensorflow/tensorboard/issues/4784)\n",
    "%tensorboard --logdir=$logdir --port=$rand_port --load_fast=false\n",
    "\n",
    "print(f\"Go to https://jupyter.lisa.surfsara.nl/jhlsrf016/user/{username}/proxy/{rand_port}/#pytorch_profiler\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
